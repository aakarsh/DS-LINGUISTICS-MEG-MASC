@article{KING2014203,
title = {Characterizing the dynamics of mental representations: the temporal generalization method},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {4},
pages = {203-210},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314000199},
author = {J-R. King and S. Dehaene},
keywords = {decoding, EEG, MEG, temporal generalization, serial processing, parallel processing, multivariate pattern analyses},
abstract = {Parsing a cognitive task into a sequence of operations is a central problem in cognitive neuroscience. We argue that a major advance is now possible owing to the application of pattern classifiers to time-resolved recordings of brain activity [electroencephalography (EEG), magnetoencephalography (MEG), or intracranial recordings]. By testing at which moment a specific mental content becomes decodable in brain activity, we can characterize the time course of cognitive codes. Most importantly, the manner in which the trained classifiers generalize across time, and from one experimental condition to another, sheds light on the temporal organization of information-processing stages. A repertoire of canonical dynamical patterns is observed across various experiments and brain regions. This method thus provides a novel way to understand how mental representations are manipulated and transformed.}
}
@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162/",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543"
}

@article{Honnibal_spaCy_Industrial-strength_Natural_2020,author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},doi = {10.5281/zenodo.1212303},title = {{spaCy: Industrial-strength Natural Language Processing in Python}},year = {2020}}

@article {King2020.03.05.976936,
	author = {King, Jean-R{\'e}mi and Charton, Fran{\c c}ois and Lopez-Paz, David and Oquab, Maxime},
	title = {Discriminating the Influence of Correlated Factors from Multivariate Observations: the Back-to-Back Regression},
	elocation-id = {2020.03.05.976936},
	year = {2020},
	doi = {10.1101/2020.03.05.976936},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Identifying causes solely from observations can be particularly challenging when i) potential factors are difficult to manipulate independently and ii) observations are multi-dimensional. To address this issue, we introduce {\textquotedblleft}Back-to-Back{\textquotedblright} regression (B2B), a linear method designed to efficiently measure, from a set of correlated factors, those that most plausibly account for multidimensional observations. First, we prove the consistency of B2B, its links to other linear approaches, and show how it provides a robust, unbiased and interpretable scalar estimate for each factor. Second, we use a variety of simulated data to show that B2B outperforms least-squares regression and cross-decomposition techniques (e.g. canonical correlation analysis and partial least squares) on causal identification when the factors and the observations are partially collinear. Finally, we apply B2B to magneto-encephalography of 102 subjects recorded during a reading task to test whether our method appropriately disentangles the respective contribution of word length and word frequency - two correlated factors known to cause early and late brain responses respectively. The results show that these two factors are better disentangled with B2B than with other standard techniques.},
	URL = {https://www.biorxiv.org/content/early/2020/03/06/2020.03.05.976936},
	eprint = {https://www.biorxiv.org/content/early/2020/03/06/2020.03.05.976936.full.pdf},
	journal = {bioRxiv}
}



@article{gwilliams2020neural, title={Neural dynamics of phoneme sequencing in real speech jointly encode order and invariant content}, author={Gwilliams, Laura and King, Jean-Remi and Marantz, Alec and Poeppel, David}, journal={BioRxiv}, year={2020}, publisher={Cold Spring Harbor Laboratory},
url={https://github.com/kingjr/meg-masc}
}

@article {Gwilliams2024.04.19.590280,
	author = {Gwilliams, Laura and Marantz, Alec and Poeppel, David and King, Jean-Remi},
	title = {Hierarchical dynamic coding coordinates speech comprehension in the brain},
	elocation-id = {2024.04.19.590280},
	year = {2024},
	doi = {10.1101/2024.04.19.590280},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Speech comprehension requires the human brain to transform an acoustic waveform into meaning. To do so, the brain generates a hierarchy of features that converts the sensory input into increasingly abstract language properties. However, little is known about how these hierarchical features are generated and continuously coordinated. Here, we propose that each linguistic feature is dynamically represented in the brain to simultaneously represent successive events. To test this {\textquoteleft}Hierarchical Dynamic Coding{\textquoteright} (HDC) hypothesis, we use time-resolved decoding of brain activity to track the construction, maintenance, and integration of a comprehensive hierarchy of language features spanning acoustic, phonetic, sub-lexical, lexical, syntactic and semantic representations. For this, we recorded 21 participants with magnetoencephalography (MEG), while they listened to two hours of short stories. Our analyses reveal three main findings. First, the brain incrementally represents and simultaneously maintains successive features. Second, the duration of these representations depend on their level in the language hierarchy. Third, each representation is maintained by a dynamic neural code, which evolves at a speed commensurate with its corresponding linguistic level. This HDC preserves the maintenance of information over time while limiting the interference between successive features. Overall, HDC reveals how the human brain continuously builds and maintains a language hierarchy during natural speech comprehension, thereby anchoring linguistic theories to their biological implementations.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2024/04/19/2024.04.19.590280},
	eprint = {https://www.biorxiv.org/content/early/2024/04/19/2024.04.19.590280.full.pdf},
	journal = {bioRxiv}
}


@misc{gwilliams2022megmaschighqualitymagnetoencephalographydataset,
      title={MEG-MASC: a high-quality magneto-encephalography dataset for evaluating natural speech processing}, 
      author={Laura Gwilliams and Graham Flick and Alec Marantz and Liina Pylkkanen and David Poeppel and Jean-Remi King},
      year={2022},
      eprint={2208.11488},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/2208.11488}, 
}

@article{Gwilliams7585,
	author = {Gwilliams, Laura and Linzen, Tal and Poeppel, David and Marantz, Alec},
	title = {In Spoken Word Recognition, the Future Predicts the Past},
	volume = {38},
	number = {35},
	pages = {7585--7599},
	year = {2018},
	doi = {10.1523/JNEUROSCI.0065-18.2018},
	publisher = {Society for Neuroscience},
	abstract = {Speech is an inherently noisy and ambiguous signal. To fluently derive meaning, a listener must integrate contextual information to guide interpretations of the sensory input. Although many studies have demonstrated the influence of prior context on speech perception, the neural mechanisms supporting the integration of subsequent context remain unknown. Using MEG to record from human auditory cortex, we analyzed responses to spoken words with a varyingly ambiguous onset phoneme, the identity of which is later disambiguated at the lexical uniqueness point. Fifty participants (both male and female) were recruited across two MEG experiments. Our findings suggest that primary auditory cortex is sensitive to phonological ambiguity very early during processing at just 50 ms after onset. Subphonemic detail is preserved in auditory cortex over long timescales and re-evoked at subsequent phoneme positions. Commitments to phonological categories occur in parallel, resolving on the shorter timescale of \~{}450 ms. These findings provide evidence that future input determines the perception of earlier speech sounds by maintaining sensory features until they can be integrated with top-down lexical information.SIGNIFICANCE STATEMENT The perception of a speech sound is determined by its surrounding context in the form of words, sentences, and other speech sounds. Often, such contextual information becomes available later than the sensory input. The present study is the first to unveil how the brain uses this subsequent information to aid speech comprehension. Concretely, we found that the auditory system actively maintains the acoustic signal in auditory cortex while concurrently making guesses about the identity of the words being said. Such a processing strategy allows the content of the message to be accessed quickly while also permitting reanalysis of the acoustic signal to minimize parsing mistakes.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/38/35/7585},
	eprint = {https://www.jneurosci.org/content/38/35/7585.full.pdf},
	journal = {Journal of Neuroscience}
}
